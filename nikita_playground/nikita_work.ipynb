{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         name      id  minutes  \\\n",
      "0  arriba   baked winter squash mexican style  137739       55   \n",
      "1            a bit different  breakfast pizza   31490       30   \n",
      "2                   all in the kitchen  chili  112140      130   \n",
      "3                          alouette  potatoes   59389       45   \n",
      "4          amish  tomato ketchup  for canning   44061      190   \n",
      "\n",
      "   contributor_id   submitted  \\\n",
      "0           47892  2005-09-16   \n",
      "1           26278  2002-06-17   \n",
      "2          196586  2005-02-25   \n",
      "3           68585  2003-04-14   \n",
      "4           41706  2002-10-25   \n",
      "\n",
      "                                                tags  \\\n",
      "0  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
      "1  ['30-minutes-or-less', 'time-to-make', 'course...   \n",
      "2  ['time-to-make', 'course', 'preparation', 'mai...   \n",
      "3  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
      "4  ['weeknight', 'time-to-make', 'course', 'main-...   \n",
      "\n",
      "                                    nutrition  n_steps  \\\n",
      "0       [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]       11   \n",
      "1   [173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]        9   \n",
      "2  [269.8, 22.0, 32.0, 48.0, 39.0, 27.0, 5.0]        6   \n",
      "3   [368.1, 17.0, 10.0, 2.0, 14.0, 8.0, 20.0]       11   \n",
      "4   [352.9, 1.0, 337.0, 23.0, 3.0, 0.0, 28.0]        5   \n",
      "\n",
      "                                               steps  \\\n",
      "0  ['make a choice and proceed with recipe', 'dep...   \n",
      "1  ['preheat oven to 425 degrees f', 'press dough...   \n",
      "2  ['brown ground beef in large pot', 'add choppe...   \n",
      "3  ['place potatoes in a large pot of lightly sal...   \n",
      "4  ['mix all ingredients& boil for 2 1 / 2 hours ...   \n",
      "\n",
      "                                         description  \\\n",
      "0  autumn is my favorite time of year to cook! th...   \n",
      "1  this recipe calls for the crust to be prebaked...   \n",
      "2  this modified version of 'mom's' chili was a h...   \n",
      "3  this is a super easy, great tasting, make ahea...   \n",
      "4  my dh's amish mother raised him on this recipe...   \n",
      "\n",
      "                                         ingredients  n_ingredients  \n",
      "0  ['winter squash', 'mexican seasoning', 'mixed ...              7  \n",
      "1  ['prepared pizza crust', 'sausage patty', 'egg...              6  \n",
      "2  ['ground beef', 'yellow onions', 'diced tomato...             13  \n",
      "3  ['spreadable cheese with garlic and herbs', 'n...             11  \n",
      "4  ['tomato juice', 'apple cider vinegar', 'sugar...              8  \n",
      "      id      cuisine                                        ingredients\n",
      "0  10259        greek  [romaine lettuce, black olives, grape tomatoes...\n",
      "1  25693  southern_us  [plain flour, ground pepper, salt, tomatoes, g...\n",
      "2  20130     filipino  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
      "3  22213       indian                [water, vegetable oil, wheat, salt]\n",
      "4  13162       indian  [black pepper, shallots, cornflour, cayenne pe...\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load CSV file\n",
    "raw_recipes = pd.read_csv(\"RAW_recipes.csv\")\n",
    "\n",
    "# Load JSON file\n",
    "with open(\"train.json\", \"r\") as file:\n",
    "    train_recipes = json.load(file)  # Loads as a list of dicts\n",
    "    train_recipes = pd.DataFrame(train_recipes)  # Convert to DataFrame\n",
    "\n",
    "# View Data\n",
    "print(raw_recipes.head())\n",
    "print(train_recipes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "\n",
    "raw_recipes = raw_recipes[['name', 'ingredients', 'steps', 'minutes', 'tags', 'nutrition']]\n",
    "train_recipes = train_recipes[['id', 'ingredients']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Ingredients Lists into a Standard Format\n",
    "\n",
    "import ast\n",
    "\n",
    "def clean_ingredients(ingredient_list):\n",
    "    \"\"\"Convert stringified list into an actual Python list.\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(ingredient_list)  # Convert string to list\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []  # Return empty list if there's an issue\n",
    "\n",
    "# Apply function to datasets\n",
    "raw_recipes['ingredients'] = raw_recipes['ingredients'].apply(clean_ingredients)\n",
    "train_recipes['ingredients'] = train_recipes['ingredients'].apply(clean_ingredients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Ingredients for NLP Processing\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_ingredients(ingredient_list):\n",
    "    \"\"\"Clean and normalize ingredient names.\"\"\"\n",
    "    normalized = []\n",
    "    for ingredient in ingredient_list:\n",
    "        ingredient = ingredient.lower()  # Lowercase\n",
    "        ingredient = re.sub(r\"\\(.*?\\)\", \"\", ingredient)  # Remove parentheses\n",
    "        ingredient = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", ingredient)  # Remove special chars\n",
    "        ingredient = ingredient.strip()\n",
    "        normalized.append(ingredient)\n",
    "    return normalized\n",
    "\n",
    "# Apply normalization\n",
    "raw_recipes['ingredients'] = raw_recipes['ingredients'].apply(normalize_ingredients)\n",
    "train_recipes['ingredients'] = train_recipes['ingredients'].apply(normalize_ingredients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Cooking Time\n",
    "\n",
    "raw_recipes['minutes'] = pd.to_numeric(raw_recipes['minutes'], errors='coerce')\n",
    "raw_recipes = raw_recipes.dropna(subset=['minutes'])  # Drop invalid entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'punkt' tokenizer found! Loading manually...\n",
      "['Hello.', 'This is a test sentence.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "\n",
    "nltk.data.path.insert(0, \"/Users/nikitaudayshinde/nltk_data\")\n",
    "punkt_path = \"/Users/nikitaudayshinde/nltk_data/tokenizers/punkt/english.pickle\"\n",
    "\n",
    "if os.path.exists(punkt_path):\n",
    "    print(\"✅ 'punkt' tokenizer found! Loading manually...\")\n",
    "\n",
    "    with open(punkt_path, \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "\n",
    "    nltk.tokenize.sent_tokenize = tokenizer.tokenize\n",
    "else:\n",
    "    print(\"❌ 'punkt' tokenizer NOT found! Check the path.\")\n",
    "\n",
    "# Verify tokenizer works now:\n",
    "text = \"Hello. This is a test sentence.\"\n",
    "print(nltk.tokenize.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Recipe Steps (For NLP)\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Tokenize steps into sentences\n",
    "raw_recipes['steps'] = raw_recipes['steps'].apply(lambda x: sent_tokenize(x) if isinstance(x, str) else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the Processed Data Locally\n",
    "\n",
    "raw_recipes.to_csv(\"processed_recipes.csv\", index=False)\n",
    "train_recipes.to_json(\"processed_train.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
