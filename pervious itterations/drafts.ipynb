{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         name      id  minutes  \\\n",
      "0  arriba   baked winter squash mexican style  137739       55   \n",
      "1            a bit different  breakfast pizza   31490       30   \n",
      "2                   all in the kitchen  chili  112140      130   \n",
      "3                          alouette  potatoes   59389       45   \n",
      "4          amish  tomato ketchup  for canning   44061      190   \n",
      "\n",
      "   contributor_id   submitted  \\\n",
      "0           47892  2005-09-16   \n",
      "1           26278  2002-06-17   \n",
      "2          196586  2005-02-25   \n",
      "3           68585  2003-04-14   \n",
      "4           41706  2002-10-25   \n",
      "\n",
      "                                                tags  \\\n",
      "0  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
      "1  ['30-minutes-or-less', 'time-to-make', 'course...   \n",
      "2  ['time-to-make', 'course', 'preparation', 'mai...   \n",
      "3  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
      "4  ['weeknight', 'time-to-make', 'course', 'main-...   \n",
      "\n",
      "                                    nutrition  n_steps  \\\n",
      "0       [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]       11   \n",
      "1   [173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]        9   \n",
      "2  [269.8, 22.0, 32.0, 48.0, 39.0, 27.0, 5.0]        6   \n",
      "3   [368.1, 17.0, 10.0, 2.0, 14.0, 8.0, 20.0]       11   \n",
      "4   [352.9, 1.0, 337.0, 23.0, 3.0, 0.0, 28.0]        5   \n",
      "\n",
      "                                               steps  \\\n",
      "0  ['make a choice and proceed with recipe', 'dep...   \n",
      "1  ['preheat oven to 425 degrees f', 'press dough...   \n",
      "2  ['brown ground beef in large pot', 'add choppe...   \n",
      "3  ['place potatoes in a large pot of lightly sal...   \n",
      "4  ['mix all ingredients& boil for 2 1 / 2 hours ...   \n",
      "\n",
      "                                         description  \\\n",
      "0  autumn is my favorite time of year to cook! th...   \n",
      "1  this recipe calls for the crust to be prebaked...   \n",
      "2  this modified version of 'mom's' chili was a h...   \n",
      "3  this is a super easy, great tasting, make ahea...   \n",
      "4  my dh's amish mother raised him on this recipe...   \n",
      "\n",
      "                                         ingredients  n_ingredients  \n",
      "0  ['winter squash', 'mexican seasoning', 'mixed ...              7  \n",
      "1  ['prepared pizza crust', 'sausage patty', 'egg...              6  \n",
      "2  ['ground beef', 'yellow onions', 'diced tomato...             13  \n",
      "3  ['spreadable cheese with garlic and herbs', 'n...             11  \n",
      "4  ['tomato juice', 'apple cider vinegar', 'sugar...              8  \n",
      "      id      cuisine                                        ingredients\n",
      "0  10259        greek  [romaine lettuce, black olives, grape tomatoes...\n",
      "1  25693  southern_us  [plain flour, ground pepper, salt, tomatoes, g...\n",
      "2  20130     filipino  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
      "3  22213       indian                [water, vegetable oil, wheat, salt]\n",
      "4  13162       indian  [black pepper, shallots, cornflour, cayenne pe...\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load CSV file\n",
    "raw_recipes = pd.read_csv(\"RAW_recipes.csv\")\n",
    "\n",
    "# Load JSON file\n",
    "with open(\"train.json\", \"r\") as file:\n",
    "    train_recipes = json.load(file)  # Loads as a list of dicts\n",
    "    train_recipes = pd.DataFrame(train_recipes)  # Convert to DataFrame\n",
    "\n",
    "# View Data\n",
    "print(raw_recipes.head())\n",
    "print(train_recipes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "\n",
    "raw_recipes = raw_recipes[['name', 'ingredients', 'steps', 'minutes', 'tags', 'nutrition']]\n",
    "train_recipes = train_recipes[['id', 'ingredients']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Ingredients Lists into a Standard Format\n",
    "\n",
    "import ast\n",
    "\n",
    "def clean_ingredients(ingredient_list):\n",
    "    \"\"\"Convert stringified list into an actual Python list.\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(ingredient_list)  # Convert string to list\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []  # Return empty list if there's an issue\n",
    "\n",
    "# Apply function to datasets\n",
    "raw_recipes['ingredients'] = raw_recipes['ingredients'].apply(clean_ingredients)\n",
    "train_recipes['ingredients'] = train_recipes['ingredients'].apply(clean_ingredients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Ingredients for NLP Processing\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_ingredients(ingredient_list):\n",
    "    \"\"\"Clean and normalize ingredient names.\"\"\"\n",
    "    normalized = []\n",
    "    for ingredient in ingredient_list:\n",
    "        ingredient = ingredient.lower()  # Lowercase\n",
    "        ingredient = re.sub(r\"\\(.*?\\)\", \"\", ingredient)  # Remove parentheses\n",
    "        ingredient = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", ingredient)  # Remove special chars\n",
    "        ingredient = ingredient.strip()\n",
    "        normalized.append(ingredient)\n",
    "    return normalized\n",
    "\n",
    "# Apply normalization\n",
    "raw_recipes['ingredients'] = raw_recipes['ingredients'].apply(normalize_ingredients)\n",
    "train_recipes['ingredients'] = train_recipes['ingredients'].apply(normalize_ingredients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Cooking Time\n",
    "\n",
    "raw_recipes['minutes'] = pd.to_numeric(raw_recipes['minutes'], errors='coerce')\n",
    "raw_recipes = raw_recipes.dropna(subset=['minutes'])  # Drop invalid entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'punkt' tokenizer found! Loading manually...\n",
      "['Hello.', 'This is a test sentence.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "\n",
    "nltk.data.path.insert(0, \"/Users/nikitaudayshinde/nltk_data\")\n",
    "punkt_path = \"/Users/nikitaudayshinde/nltk_data/tokenizers/punkt/english.pickle\"\n",
    "\n",
    "if os.path.exists(punkt_path):\n",
    "    print(\"✅ 'punkt' tokenizer found! Loading manually...\")\n",
    "\n",
    "    with open(punkt_path, \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "\n",
    "    nltk.tokenize.sent_tokenize = tokenizer.tokenize\n",
    "else:\n",
    "    print(\"❌ 'punkt' tokenizer NOT found! Check the path.\")\n",
    "\n",
    "# Verify tokenizer works now:\n",
    "text = \"Hello. This is a test sentence.\"\n",
    "print(nltk.tokenize.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Recipe Steps (For NLP)\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Tokenize steps into sentences\n",
    "raw_recipes['steps'] = raw_recipes['steps'].apply(lambda x: sent_tokenize(x) if isinstance(x, str) else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the Processed Data Locally\n",
    "# these are to large to push\n",
    "\n",
    "raw_recipes.to_csv(\"processed_recipes.csv\", index=False)\n",
    "train_recipes.to_json(\"processed_train.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Load CSV file\n",
    "csv_path = \"RAW_recipes.csv\"\n",
    "\n",
    "try:\n",
    "    raw_recipes = pd.read_csv(csv_path)\n",
    "except FileNotFoundError:\n",
    "    raw_recipes = None\n",
    "\n",
    "# Feature Engineering Functions\n",
    "\n",
    "def clean_ingredients(ingredient_list):\n",
    "    \"\"\"Convert stringified list into an actual Python list.\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(ingredient_list)  # Convert string to list\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []  # Return empty list if there's an issue\n",
    "\n",
    "def normalize_ingredients(ingredient_list):\n",
    "    \"\"\"Clean and normalize ingredient names.\"\"\"\n",
    "    normalized = []\n",
    "    for ingredient in ingredient_list:\n",
    "        ingredient = ingredient.lower()  # Lowercase\n",
    "        ingredient = re.sub(r\"\\(.*?\\)\", \"\", ingredient)  # Remove parentheses\n",
    "        ingredient = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", ingredient)  # Remove special chars\n",
    "        ingredient = ingredient.strip()\n",
    "        normalized.append(ingredient)\n",
    "    return normalized\n",
    "\n",
    "if raw_recipes is not None:\n",
    "    # Selecting relevant columns\n",
    "    raw_recipes = raw_recipes[['name', 'ingredients', 'steps', 'minutes', 'tags', 'nutrition']]\n",
    "\n",
    "    # Convert ingredients and steps into structured lists\n",
    "    raw_recipes['ingredients'] = raw_recipes['ingredients'].apply(clean_ingredients)\n",
    "    raw_recipes['ingredients'] = raw_recipes['ingredients'].apply(normalize_ingredients)\n",
    "\n",
    "    # Extract primary ingredient (first in list as a heuristic)\n",
    "    raw_recipes['primary_ingredient'] = raw_recipes['ingredients'].apply(lambda x: x[0] if x else \"unknown\")\n",
    "\n",
    "    # Tokenizing Steps (Simple Word Tokenization)\n",
    "    raw_recipes['tokenized_steps'] = raw_recipes['steps'].apply(lambda x: re.findall(r'\\b\\w+\\b', str(x).lower()))\n",
    "\n",
    "    # Extract Cooking Time Categories\n",
    "    def categorize_time(minutes):\n",
    "        \"\"\"Categorize recipes based on time.\"\"\"\n",
    "        if minutes < 15:\n",
    "            return \"quick\"\n",
    "        elif minutes < 30:\n",
    "            return \"moderate\"\n",
    "        elif minutes < 60:\n",
    "            return \"long\"\n",
    "        else:\n",
    "            return \"very long\"\n",
    "\n",
    "    raw_recipes['time_category'] = raw_recipes['minutes'].apply(categorize_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Recipes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>steps</th>\n",
       "      <th>minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>250 chocolate chip cookies recipe</td>\n",
       "      <td>[butter, brown sugar, vanilla, blended oatmeal...</td>\n",
       "      <td>['blended oatmeal: measure and blend in a blen...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49386</th>\n",
       "      <td>chocolate bar cake</td>\n",
       "      <td>[chocolate bars, butter, boiling water, flour,...</td>\n",
       "      <td>['heat oven to 350 degrees f', 'grease and flo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60709</th>\n",
       "      <td>cranberry orange cookies   jar mix</td>\n",
       "      <td>[light brown sugar, sugar, flour, baking soda,...</td>\n",
       "      <td>['jarmix instructions', 'in 1-qt jar , place b...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55856</th>\n",
       "      <td>coconut orange squares</td>\n",
       "      <td>[butter, sugar, egg, orange rind, milk, coconu...</td>\n",
       "      <td>['crem butter and sugar until light', 'beat in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55495</th>\n",
       "      <td>coconut brownies</td>\n",
       "      <td>[butter, sugar, eggs, vanilla, flour, unsweete...</td>\n",
       "      <td>['melt butter over low heat', 'then remove', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     name  \\\n",
       "128     250 chocolate chip cookies recipe   \n",
       "49386                  chocolate bar cake   \n",
       "60709  cranberry orange cookies   jar mix   \n",
       "55856              coconut orange squares   \n",
       "55495                    coconut brownies   \n",
       "\n",
       "                                             ingredients  \\\n",
       "128    [butter, brown sugar, vanilla, blended oatmeal...   \n",
       "49386  [chocolate bars, butter, boiling water, flour,...   \n",
       "60709  [light brown sugar, sugar, flour, baking soda,...   \n",
       "55856  [butter, sugar, egg, orange rind, milk, coconu...   \n",
       "55495  [butter, sugar, eggs, vanilla, flour, unsweete...   \n",
       "\n",
       "                                                   steps  minutes  \n",
       "128    ['blended oatmeal: measure and blend in a blen...        6  \n",
       "49386  ['heat oven to 350 degrees f', 'grease and flo...        0  \n",
       "60709  ['jarmix instructions', 'in 1-qt jar , place b...       10  \n",
       "55856  ['crem butter and sugar until light', 'beat in...        0  \n",
       "55495  ['melt butter over low heat', 'then remove', '...        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Load Recipes Data\n",
    "csv_path = \"RAW_recipes.csv\"  # Update the path if needed\n",
    "\n",
    "try:\n",
    "    raw_recipes = pd.read_csv(csv_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: RAW_recipes.csv not found. Please check your file path.\")\n",
    "\n",
    "# Convert stringified ingredient lists into actual lists\n",
    "def clean_ingredients(ingredient_list):\n",
    "    \"\"\"Convert stringified list into an actual Python list.\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(ingredient_list)  # Convert string to list\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []  # Return empty list if there's an issue\n",
    "\n",
    "# Normalize ingredient text\n",
    "def normalize_ingredients(ingredient_list):\n",
    "    \"\"\"Clean and normalize ingredient names.\"\"\"\n",
    "    normalized = []\n",
    "    for ingredient in ingredient_list:\n",
    "        ingredient = ingredient.lower()  # Lowercase\n",
    "        ingredient = re.sub(r\"\\(.*?\\)\", \"\", ingredient)  # Remove parentheses\n",
    "        ingredient = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", ingredient)  # Remove special chars\n",
    "        ingredient = ingredient.strip()\n",
    "        normalized.append(ingredient)\n",
    "    return normalized\n",
    "\n",
    "# Data Cleaning\n",
    "if raw_recipes is not None:\n",
    "    raw_recipes = raw_recipes[['name', 'ingredients', 'steps', 'minutes', 'tags', 'nutrition']]\n",
    "\n",
    "    # Process ingredients\n",
    "    raw_recipes['ingredients'] = raw_recipes['ingredients'].apply(clean_ingredients)\n",
    "    raw_recipes['ingredients'] = raw_recipes['ingredients'].apply(normalize_ingredients)\n",
    "\n",
    "    # Extract time categories\n",
    "    def categorize_time(minutes):\n",
    "        \"\"\"Categorize recipes based on time.\"\"\"\n",
    "        if minutes < 15:\n",
    "            return \"quick\"\n",
    "        elif minutes < 30:\n",
    "            return \"moderate\"\n",
    "        elif minutes < 60:\n",
    "            return \"long\"\n",
    "        else:\n",
    "            return \"very long\"\n",
    "\n",
    "    raw_recipes['time_category'] = raw_recipes['minutes'].apply(categorize_time)\n",
    "\n",
    "# Rule-Based Recipe Search Function\n",
    "def find_recipes(user_ingredients, baking_method=None, time_category=None):\n",
    "    \"\"\"\n",
    "    Rule-based recipe search.\n",
    "    Filters recipes based on user-provided ingredients, optional baking method, and time constraint.\n",
    "    \"\"\"\n",
    "    if raw_recipes is None:\n",
    "        print(\"Error: Recipe data not loaded.\")\n",
    "        return None\n",
    "\n",
    "    # Convert user ingredients to lowercase for matching\n",
    "    user_ingredients = [ing.lower() for ing in user_ingredients]\n",
    "\n",
    "    # Filter recipes that contain at least one user-provided ingredient\n",
    "    def ingredient_match(recipe_ingredients):\n",
    "        return any(ing in recipe_ingredients for ing in user_ingredients)\n",
    "\n",
    "    filtered_recipes = raw_recipes[raw_recipes[\"ingredients\"].apply(ingredient_match)]\n",
    "\n",
    "    # If a baking method is provided, filter by method in steps\n",
    "    if baking_method:\n",
    "        filtered_recipes = filtered_recipes[\n",
    "            filtered_recipes[\"steps\"].str.contains(baking_method, case=False, na=False)\n",
    "        ]\n",
    "\n",
    "    # If a time category is provided, filter by time constraint\n",
    "    if time_category:\n",
    "        filtered_recipes = filtered_recipes[filtered_recipes[\"time_category\"] == time_category]\n",
    "\n",
    "    # Sort recipes by the number of matching ingredients\n",
    "    filtered_recipes[\"ingredient_match_count\"] = filtered_recipes[\"ingredients\"].apply(\n",
    "        lambda recipe_ing: sum(ing in recipe_ing for ing in user_ingredients)\n",
    "    )\n",
    "    filtered_recipes = filtered_recipes.sort_values(by=\"ingredient_match_count\", ascending=False)\n",
    "\n",
    "    # Select top results\n",
    "    top_results = filtered_recipes[[\"name\", \"ingredients\", \"steps\", \"minutes\"]].head(5)\n",
    "    \n",
    "    return top_results\n",
    "\n",
    "# Example Usage\n",
    "user_ingredients = [\"flour\", \"sugar\", \"butter\"]\n",
    "baking_method = \"bake\"\n",
    "time_category = \"quick\"\n",
    "\n",
    "recommended_recipes = find_recipes(user_ingredients, baking_method, time_category)\n",
    "\n",
    "\n",
    "if recommended_recipes is not None:\n",
    "    print(\"Recommended Recipes:\")\n",
    "    display(recommended_recipes)  # Use Pandas' built-in display function in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP_recipes Columns: Index(['id', 'i', 'name_tokens', 'ingredient_tokens', 'steps_tokens',\n",
      "       'techniques', 'calorie_level', 'ingredient_ids'],\n",
      "      dtype='object')\n",
      "RAW_recipes Columns: Index(['name', 'id', 'minutes', 'contributor_id', 'submitted', 'tags',\n",
      "       'nutrition', 'n_steps', 'steps', 'description', 'ingredients',\n",
      "       'n_ingredients'],\n",
      "      dtype='object')\n",
      "Test_Recipes Columns: Index(['id', 'ingredients'], dtype='object')\n",
      "Train_Recipes Columns: Index(['id', 'cuisine', 'ingredients'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/A4/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'steps'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize_steps\u001b[39m(steps):\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [sent.text \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m nlp(steps).sents]\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33msteps\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msteps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.apply(tokenize_steps)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# unit mapping\u001b[39;00m\n\u001b[32m     53\u001b[39m unit_mapping = {\n\u001b[32m     54\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtbsp\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtablespoon\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     55\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtbs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtablespoon\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mkg\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mkilogram\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     60\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/A4/venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/A4/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'steps'"
     ]
    }
   ],
   "source": [
    "# Converts The Full Recipe Dataset\n",
    "#  \n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets from local files\n",
    "df1 = pd.read_csv(\"PP_recipes.csv\")\n",
    "df2 = pd.read_csv(\"RAW_recipes.csv\")\n",
    "df3 = pd.read_json(\"test.json\")\n",
    "df4 = pd.read_json(\"train.json\")\n",
    "\n",
    "# Store datasets in a dictionary for easier access\n",
    "datasets = {\n",
    "    \"PP_recipes\": df1,\n",
    "    \"RAW_recipes\": df2,\n",
    "    \"Test_Recipes\": df3,\n",
    "    \"Train_Recipes\": df4\n",
    "}\n",
    "\n",
    "# Print dataset column names to understand their structure\n",
    "for name, df in datasets.items():\n",
    "    print(f\"{name} Columns:\", df.columns)\n",
    "# unit mapping\n",
    "unit_mapping = {\n",
    "    \"tbsp\": \"tablespoon\",\n",
    "    \"tbs\": \"tablespoon\",\n",
    "    \"tsp\": \"teaspoon\",\n",
    "    \"oz\": \"ounce\",\n",
    "    \"g\": \"gram\",\n",
    "    \"kg\": \"kilogram\",\n",
    "}\n",
    "\n",
    "def standardize_units(ingredient):\n",
    "    words = ingredient.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word in unit_mapping:\n",
    "            words[i] = unit_mapping[word]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"ingredients\"] = df[\"ingredients\"].apply(lambda x: [standardize_units(i) for i in x])\n",
    "\n",
    "# tokenizing\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_steps(steps):\n",
    "    return [sent.text for sent in nlp(steps).sents]\n",
    "\n",
    "df[\"steps\"] = df[\"steps\"].apply(tokenize_steps)\n",
    "\n",
    "\n",
    "# unit mapping\n",
    "unit_mapping = {\n",
    "    \"tbsp\": \"tablespoon\",\n",
    "    \"tbs\": \"tablespoon\",\n",
    "    \"tsp\": \"teaspoon\",\n",
    "    \"oz\": \"ounce\",\n",
    "    \"g\": \"gram\",\n",
    "    \"kg\": \"kilogram\",\n",
    "}\n",
    "\n",
    "def standardize_units(ingredient):\n",
    "    words = ingredient.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word in unit_mapping:\n",
    "            words[i] = unit_mapping[word]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"ingredients\"] = df[\"ingredients\"].apply(lambda x: [standardize_units(i) for i in x])\n",
    "\n",
    "# tokenizing\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_steps(steps):\n",
    "    return [sent.text for sent in nlp(steps).sents]\n",
    "\n",
    "df[\"steps\"] = df[\"steps\"].apply(tokenize_steps)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())  # View the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"RAW_recipes.csv\")  # Ensure you're using the right file\n",
    "print(df.columns)  # Check available columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import spacy\n",
    "\n",
    "# ✅ Load the correct dataset\n",
    "df = pd.read_csv(\"RAW_recipes.csv\")  # Make sure this is the right file\n",
    "\n",
    "# ✅ Ensure all necessary columns are present\n",
    "required_columns = {\"name\", \"ingredients\", \"steps\", \"minutes\"}\n",
    "missing_columns = required_columns - set(df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing columns in dataset: {missing_columns}\")\n",
    "\n",
    "# ✅ Load spaCy model (Make sure you installed it: `python -m spacy download en_core_web_sm`)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ✅ Function to clean and normalize ingredients\n",
    "def clean_ingredients(ingredient_list):\n",
    "    \"\"\"Convert stringified list into an actual Python list.\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(ingredient_list)  # Convert string to list\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []  # Return empty list if there's an issue\n",
    "\n",
    "def normalize_ingredients(ingredient_list):\n",
    "    \"\"\"Clean and normalize ingredient names.\"\"\"\n",
    "    normalized = []\n",
    "    for ingredient in ingredient_list:\n",
    "        ingredient = ingredient.lower().strip()  # Lowercase & Trim\n",
    "        ingredient = re.sub(r\"\\(.*?\\)\", \"\", ingredient)  # Remove text in parentheses\n",
    "        ingredient = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", ingredient)  # Remove special characters\n",
    "        normalized.append(ingredient)\n",
    "    return normalized\n",
    "\n",
    "# ✅ Function to tokenize recipe steps\n",
    "def tokenize_steps(steps):\n",
    "    \"\"\"Tokenize recipe steps into sentences.\"\"\"\n",
    "    if pd.isnull(steps) or not isinstance(steps, str):  # Handle NaN values\n",
    "        return []\n",
    "    return [sent.text for sent in nlp(steps).sents]\n",
    "\n",
    "# ✅ Apply preprocessing functions\n",
    "df[\"ingredients\"] = df[\"ingredients\"].apply(clean_ingredients)\n",
    "df[\"ingredients\"] = df[\"ingredients\"].apply(normalize_ingredients)\n",
    "\n",
    "# ✅ Ensure \"steps\" is a string before tokenizing\n",
    "df[\"steps\"] = df[\"steps\"].astype(str)\n",
    "df[\"tokenized_steps\"] = df[\"steps\"].apply(tokenize_steps)\n",
    "\n",
    "# ✅ Extract time categories\n",
    "def categorize_time(minutes):\n",
    "    \"\"\"Categorize recipes based on time required.\"\"\"\n",
    "    if minutes < 15:\n",
    "        return \"quick\"\n",
    "    elif minutes < 30:\n",
    "        return \"moderate\"\n",
    "    elif minutes < 60:\n",
    "        return \"long\"\n",
    "    else:\n",
    "        return \"very long\"\n",
    "\n",
    "df[\"time_category\"] = df[\"minutes\"].apply(categorize_time)\n",
    "\n",
    "# ✅ Display processed data\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Processed Recipes Data\", dataframe=df)\n",
    "\n",
    "print(\"✅ Data preprocessing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# ✅ Load dataset\n",
    "df = pd.read_csv(\"RAW_recipes.csv\")\n",
    "\n",
    "# ✅ Load spaCy model (Disable unused features to speed up processing)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
    "\n",
    "# ✅ Convert \"steps\" column to strings\n",
    "df[\"steps\"] = df[\"steps\"].astype(str)\n",
    "\n",
    "# ✅ Process all steps in **bulk** (MUCH faster than apply)\n",
    "df[\"tokenized_steps\"] = list(nlp.pipe(df[\"steps\"], batch_size=100))\n",
    "\n",
    "print(\"✅ Tokenization completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
